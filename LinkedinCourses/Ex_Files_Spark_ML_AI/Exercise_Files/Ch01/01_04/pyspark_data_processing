import os
os.getcwd()
os.listdir('.')
emp_df = spark.read.csv('employee.txt', header=True)
emp_df.schema
emp_df.printSchema
emp_df.printSchema()
emp_df.columns
emp_df.head(3)
emp_df.take(3)
emp_df.count()
sample_df = emp_df.sample(False, 0.1)
sample_df.count()
emp_mgrs_df = emp_df.filter('salary >= 100000')
emp_mgrs.count()
emp_mgrs_df.count()
emp_mgrs_df.select('salary').show()
from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.linalg import Vectors
features_df = spark.createDataFrame([
(1, Vectors.dense([10.0, 10000.0, 1.0]),),
(2, Vectors.dense([20.0, 30000.0, 2.0]),),
(3, Vectors.dense([30.0, 40000.0, 3.0]),),
], ['id', 'features'])
feat
features.head()
features_df.head()
features_df.head(2)
features_df.take(1)
features_df.take(2)
feature_scaler = MinMaxScaler(inputCol='features', outputCol='sfeatures')
scale_model = feature_scaler.fit(features_df)
type(scale_model)
type(feature_scaler)
scale_features_df = scale_model.transform(features_df)
type(scale_features_df)
scale_features_df.type(2)
scale_features_df.take(2)
scale_features_df.select('features', 'sfeatures').show()
from pyspark.ml.feature import StandardScaler
from pyspark.ml.linalg import Vectors
features_df_2 = spark.createDataFrame(
[
(1, Vectors.dense([10.0, 10000.0, 1.0]),),
(2, Vectors.dense([20.0, 30000.0, 2.0]),),
(3, Vectors.dense([30.0, 50000.0, 10.0]),)
],
['id', 'features'])
features_df_2.head(2)
feature_stand_scaler = StandardScaler(inputCol='features',
outputCol='sfeatures', withStd=True, withMean=True)
stand_smodel_2 = feature_stand_scaler.fit(features_df_2)
stand_sfeatures_df_2 = stand_smodel_2.transform(features_df_2)
stand_sfeatures_df_2.take(1)
stand_sfeatures_df_2.show()
from pyspark.ml.feature import Bucketizer
splits = [-float('inf'), -10.0, 0.0, 10.0, float('inf')]
splits
type(splits)
b_data = [(-800.0,),(-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]
d_data
b_data
b_df = spark.createDataFrame(b_data, ['features'])
b_df.show()
bucketizer = Bucketizer(splits=splits, inputCol=
'features', outputCol='bfeatures')
bucketed_df = bucketizer.transform(b_df)
bucketed_df.show()
bucketed_df.show(1)
from pyspark.ml.feature import Tokenizer
sentences_df = spark.createDataFrame([
(1, 'this is an introduction to Spark MLlib'),
(2, 'MLlib includes libraries for classification and regression'),
(3, 'it also contains supporting tools for pipelines')],
['id', 'sentence'])
sente
sentences_df
sentences_df.show()
sentences_df.head()
sentences_df.head(2)
sentences_df.head(3)
token_sentence = Tokenizer(inputCol='sentence', outputCol='words')
tokenized_sentence = token_sentence.transform(sentences_df)
tokenized_sentence.show()
sentences_df
sentences_df.show()
from pyspark.ml.feature import HashingTF, IDF
tokenized_sentences.show()
tokenized_sentence.show()
hashingtf = HashingTF(inputCol='words', outputCol='rawfeatures',
numFeatures=20)
sentence_hashingtf = hashingtf.transform(tokenized_sentence)
sentence_hashingtf.head()
sentence_hashingtf.show()
idf = IDF(inputCol='rawfeatures', outputCol='idf_features')
idfmodel = idf.fit(sentence_hashingtf)
sentence_idf = idfmodel.transform(sentence_hashingtf)
sentence_idf.show()
sentence_idf.head()
import readline
readline.write_history_file('pyspark_data_processing')
